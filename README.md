## Ashaar

[![Huggingface Space](https://img.shields.io/badge/ğŸ¤—-Demo%20-yellow.svg)](https://huggingface.co/spaces/arbml/Ashaar)
[![Colab Notebook](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Z6c0ogy8Yt89UJgT_fAvb0xdKwfBYxK_?usp=sharing)
[![GitHub](https://img.shields.io/badge/ğŸ’»-GitHub%20-black.svg)](https://github.com/ARBML/Ashaar)

Arabic poetry analysis and generation. 

<p align = 'center'>
<img src='https://raw.githubusercontent.com/ARBML/Ashaar/master/images/ashaar_icon.png' width='150px' alt='logo for Ashaar'/>
</p>

## Installation

```
pip install .
```

## Generation 

### Training 

Training the character based gpt-2 model. Our model was trained on A100 for around 500k steps. 

```
python run_clm.py \
        --model_type gpt2 \
        --config_overrides="n_layer=10,vocab_size=100" \
        --dataset_name arbml/Ashaar_dataset \
        --tokenizer_name arbml/ashaar_tokenizer \
        --per_device_train_batch_size 16 \
        --per_device_eval_batch_size 4 \
        --do_train \
        --do_eval \
        --num_train_epochs=100 \
        --eval_steps=500 \
        --logging_steps=1 \
        --save_steps=500 \
        --logging_strategy='steps' \
        --evaluation_strategy='steps' \
        --save_strategy='steps' \
        --output_dir <output-dir> \
        --report_to="wandb" \
        --overwrite_output_dir \
        --push_to_hub \
        --hub_model_id=<model-hub-id>
```

### Inference 

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

prompt = "enter your prompt here"
gpt_tokenizer = AutoTokenizer.from_pretrained('arbml/Ashaar_tokenizer')
model = AutoModelForCausalLM.from_pretrained('arbml/Ashaar_model')

encoded_input = gpt_tokenizer(prompt, return_tensors='pt')
output = model.generate(**encoded_input, max_length = 512, top_p = 3, do_sample=True)
```

## Analysis

```python
from Ashaar.bait_analysis import BaitAnalysis

prompt ="Ø£Ù„Ø§ Ù„ÙŠØª Ø´Ø¹Ø±ÙŠ Ù‡Ù„ Ø£Ø¨ÙŠØªÙ† Ù„ÙŠÙ„Ø© # Ø¨Ø¬Ù†Ø¨ Ø§Ù„ØºØ¶Ù‰ Ø£Ø²Ø¬ÙŠ Ø§Ù„Ù‚Ù„Ø§Øµ Ø§Ù„Ù†ÙˆØ§Ø¬ÙŠØ§"
analysis = BaitAnalysis()
output = analysis.analyze(prompt, override_tashkeel=True)
```

Sample output 

```
{'diacritized': ['Ø£ÙÙ„Ø§ Ù„ÙÙŠÙ’ØªÙ Ø´ÙØ¹Ù’Ø±ÙÙŠ Ù‡ÙÙ„Ù’ Ø£ÙØ¨ÙÙŠÙ’ØªÙÙ†ÙÙ‘ Ù„ÙÙŠÙ’Ù„ÙØ©Ù‹ # Ø¨ÙØ¬ÙÙ†Ù’Ø¨Ù Ø§Ù„Ù’ØºÙØ¶ÙÙ‰ Ø£ÙØ²ÙØ¬ÙÙŠÙÙ‘ Ø§Ù„Ù’Ù‚ÙÙ„ÙØ§ØµÙ Ø§Ù„Ù†ÙÙ‘ÙˆÙØ§Ø¬ÙÙŠÙØ§'],
 'arudi_style': [('Ø£Ù„Ø§ Ù„ÙŠØª Ø´Ø¹Ø±ÙŠ Ù‡Ù„ Ø£Ø¨ÙŠØªÙ†Ù† Ù„ÙŠÙ„ØªÙ†', '10110110101011010110110'),
  ('Ø¨Ø¬Ù†Ø¨ Ù„ØºØ¶Ù‰ Ø£Ø²Ø¬ÙŠÙŠ Ù„Ù‚Ù„Ø§Øµ Ù†Ù†ÙˆØ§Ø¬ÙŠØ§', '1101011011101011010110110')],
 'patterns_mismatches': ['G1G0R1G1G0G1G1G0G1G0G1G0G1G1G0G1G0G1G1G0G1G1G0',
  'G1G1G0G1G0G1G1G0R1R1G1G0G1G0G1G1G0G1G0G1G1G0G1G1G0'],
 'qafiyah': ('ÙŠ',
  'Ù‚Ø§ÙÙŠØ© Ø¨Ø­Ø±Ù Ø§Ù„Ø±ÙˆÙŠ: ÙŠ ØŒ  Ø²Ø§Ø¯ Ù„Ù‡Ø§ Ø§Ù„ÙˆØµÙ„ Ø¨Ø¥Ø´Ø¨Ø§Ø¹ Ø±ÙˆÙŠÙ‡Ø§ Ø²Ø§Ø¯ Ù„Ù‡Ø§ Ø§Ù„ØªØ£Ø³ÙŠØ³'),
 'meter': 'Ø§Ù„Ø·ÙˆÙŠÙ„',
 'closest_baits': [[('ÙÙÙŠÙØ§ Ù„ÙÙŠÙ’ØªÙÙ†ÙÙŠ Ù„ÙÙ…Ù’ Ø£ÙØ¹Ù’Ù†Ù Ø¨ÙØ§Ù„Ù’Ù…ÙÙ„Ù’ÙƒÙ Ø³ÙØ§Ø¹ÙØ©Ù‹ # ÙˆÙÙ„ÙÙ…Ù’ Ø£ÙÙ„Ù’Ù‡Ù ÙÙÙŠ Ù„ÙØ°ÙÙ‘Ø§ØªÙ Ø¹ÙÙŠÙ’Ø´Ù Ù†ÙÙˆÙØ§Ø¶ÙØ±Ù',
    [0.8884615898132324])]],
 'era': ['Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø¬Ø§Ù‡Ù„ÙŠ', 'Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠ', 'Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø£Ù…ÙˆÙŠ', 'Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…'],
 'closest_patterns': [('1010110101011010110110',
   0.9777777777777777,
   'Ø¹ÙˆÙ„Ù†Ù’ Ù…ÙØ§Ø¹ÙŠÙ„Ù†Ù’ ÙØ¹ÙˆÙ„Ù†Ù’ Ù…ÙØ§Ø¹Ù„Ù†Ù’'),
  ('11010110101011010110110',
   0.9583333333333334,
   'ÙØ¹ÙˆÙ„Ù†Ù’ Ù…ÙØ§Ø¹ÙŠÙ„Ù†Ù’ ÙØ¹ÙˆÙ„Ù†Ù’ Ù…ÙØ§Ø¹Ù„Ù†Ù’')],
 'theme': ['Ù‚ØµÙŠØ¯Ø© Ø±ÙˆÙ…Ù†Ø³ÙŠÙ‡', 'Ù‚ØµÙŠØ¯Ø© Ø´ÙˆÙ‚', 'Ù‚ØµÙŠØ¯Ø© ØºØ²Ù„']}
```

## Citation 
```
@article{alyafeai2023ashaar,
  title={Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep Learning Approaches},
  author={Alyafeai, Zaid and Al-Shaibani, Maged S and Ahmed, Moataz},
  journal={arXiv preprint arXiv:2307.06218},
  year={2023}
}
```
